name: PR CI - CanPRDev Project Review # Workflow name, changed for Python projects | PR CI - CanPRDev å°ˆæ¡ˆå¯©æŸ¥

on:
  pull_request:
    branches:
      - main # Trigger condition: Pull Request to the main branch | è§¸ç™¼æ¢ä»¶ï¼šåœ¨ main åˆ†æ”¯çš„ Pull Request

# Environment variables configuration | ç’°å¢ƒè®Šæ•¸è¨­å®š
env:
  PYTHON_VERSION: '3.9'
  MAX_DIFF_SIZE: 6000
  OPENAI_MODEL: 'gpt-4o'
  REVIEW_TITLE: 'ğŸ¤– **AI Code Review Feedback** | AI ç¨‹å¼ç¢¼å¯©æŸ¥æ„è¦‹'

# Explicitly set permissions | æ˜ç¢ºè¨­å®šæ¬Šé™
permissions:
  contents: read
  pull-requests: write

jobs:
  build-and-test:
    name: Build and Test # Job name | ä½œæ¥­åç¨±
    runs-on: ubuntu-latest # Execution environment | åŸ·è¡Œç’°å¢ƒ

    steps:
      - name: Checkout code # Step: Checkout code | æ­¥é©Ÿï¼šæª¢å‡ºç¨‹å¼ç¢¼
        uses: actions/checkout@v3
        with:
          fetch-depth: 0 # Get the complete git history for better diff comparison | ç²å–å®Œæ•´çš„ git æ­·å²è¨˜éŒ„ï¼Œä»¥ä¾¿é€²è¡Œæ›´å¥½çš„å·®ç•°æ¯”è¼ƒ

      - name: Set up Python # Step: Set up Python environment | æ­¥é©Ÿï¼šè¨­å®š Python ç’°å¢ƒ
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }} # Use environment variable for Python version | ä½¿ç”¨ç’°å¢ƒè®Šæ•¸è¨­å®š Python ç‰ˆæœ¬

      - name: Cache pip dependencies # Step: Cache pip dependencies | æ­¥é©Ÿï¼šå¿«å– pip ä¾è³´
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies # Step: Install Python dependencies | æ­¥é©Ÿï¼šå®‰è£ Python ä¾è³´
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt # Install project dependencies | å®‰è£å°ˆæ¡ˆä¾è³´
          else
            echo "No requirements.txt found, skipping dependency installation"
          fi
          # Install necessary tools | å®‰è£å¿…è¦çš„å·¥å…·
          pip install jq

      - name: Notify Review Start # Step: Notify review start | æ­¥é©Ÿï¼šé€šçŸ¥å¯©æŸ¥é–‹å§‹
        uses: peter-evans/create-or-update-comment@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          issue-number: ${{ github.event.pull_request.number }}
          body: |
            ğŸ” AI code review process started. Please wait while I analyze your code...
            ğŸ” AI ç¨‹å¼ç¢¼å¯©æŸ¥æµç¨‹å·²å•Ÿå‹•ã€‚è«‹ç¨å€™ï¼Œæˆ‘æ­£åœ¨åˆ†ææ‚¨çš„ç¨‹å¼ç¢¼...

      - name: Get PR Diff and Determine Size # Step: Get PR diff and determine size | æ­¥é©Ÿï¼šç²å– PR å·®ç•°ä¸¦ç¢ºå®šå¤§å°
        id: diff
        run: |
          echo "Getting diff for PR #${{ github.event.pull_request.number }}"
          git fetch origin main # Fetch the latest state of the main branch | ç²å–ä¸»åˆ†æ”¯çš„æœ€æ–°ç‹€æ…‹
          git diff origin/main > diff.txt # Write the diff to diff.txt file | å°‡å·®ç•°å¯«å…¥ diff.txt æª”æ¡ˆ
          
          # Determine PR size | ç¢ºå®š PR å¤§å°
          DIFF_SIZE=$(cat diff.txt | wc -c)
          echo "diff_size=$DIFF_SIZE" >> $GITHUB_OUTPUT
          if [ $DIFF_SIZE -gt ${{ env.MAX_DIFF_SIZE }} ]; then
            echo "is_large=true" >> $GITHUB_OUTPUT
            echo "PR is large ($DIFF_SIZE bytes), will analyze most important parts only"
          else
            echo "is_large=false" >> $GITHUB_OUTPUT
            echo "PR is manageable size ($DIFF_SIZE bytes), will analyze completely"
          fi

      - name: Process Large PR # Step: Process large PR | æ­¥é©Ÿï¼šè™•ç†å¤§å‹ PR
        if: steps.diff.outputs.is_large == 'true'
        run: |
          echo "PR is too large for complete analysis. Analyzing most important files only..."
          # Find most important files (e.g., Python files, excluding test files) | å°‹æ‰¾æœ€é‡è¦çš„æª”æ¡ˆï¼ˆä¾‹å¦‚ Python æª”æ¡ˆï¼Œæ’é™¤æ¸¬è©¦æª”æ¡ˆï¼‰
          git diff --name-only origin/main | grep -E '\.py$' | grep -v '_test\.py$' | grep -v 'test_' | head -10 > important_files.txt
          
          # Extract diff only for important files | åªæå–é‡è¦æª”æ¡ˆçš„å·®ç•°
          > filtered_diff.txt
          while IFS= read -r file; do
            echo "Including $file in analysis"
            git diff origin/main -- "$file" >> filtered_diff.txt
          done < important_files.txt
          
          # Replace original diff with filtered diff | ä½¿ç”¨éæ¿¾å¾Œçš„å·®ç•°æ›¿æ›åŸå§‹å·®ç•°
          mv filtered_diff.txt diff.txt

      # Optional SonarQube scan | å¯é¸çš„ SonarQube æƒæ
      - name: SonarQube Scan
        if: ${{ vars.USE_SONARQUBE == 'true' }}
        id: sonarqube-scan
        uses: sonarsource/sonarqube-scan-action@master
        env:
          SONAR_TOKEN: ${{ secrets.SONARQUBE_TOKEN }}
          SONAR_HOST_URL: ${{ vars.SONARQUBE_URL }}
        continue-on-error: true

      - name: Call OpenAI for Review # Step: Call OpenAI for Review | æ­¥é©Ÿï¼šå‘¼å« OpenAI é€²è¡Œå¯©æŸ¥
        id: openai_review
        continue-on-error: true # Allow this step to fail but continue the workflow | å…è¨±æ­¤æ­¥é©Ÿå¤±æ•—ä½†ç¹¼çºŒå·¥ä½œæµç¨‹
        run: |
          echo "Calling OpenAI API to analyze code diff..."
          # Limit DIFF_CONTENT size to avoid exceeding OpenAI API Token limits.
          # Note: Using head -c ${{ env.MAX_DIFF_SIZE }} to limit bytes, ensuring the diff file isn't too large.
          # é™åˆ¶ DIFF_CONTENT çš„å¤§å°ï¼Œä»¥é¿å…è¶…é OpenAI API çš„ Token é™åˆ¶
          # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨ head -c ${{ env.MAX_DIFF_SIZE }} é™åˆ¶å­—ç¯€æ•¸ï¼Œç¢ºä¿ diff æª”æ¡ˆä¸æœƒå¤ªå¤§
          DIFF_CONTENT=$(cat diff.txt | head -c ${{ env.MAX_DIFF_SIZE }} | jq -Rs .)

          # Construct the JSON request data for the OpenAI API | æ§‹å»ºå‚³é€çµ¦ OpenAI API çš„ JSON è«‹æ±‚è³‡æ–™
          REQUEST_DATA=$(jq -n \
            --arg model "${{ env.OPENAI_MODEL }}" \
            --arg system_msg "You are a senior code reviewer specializing in identifying technical debt, refactoring suggestions, clean code, and SOLID principles. Please pay special attention to Python code conventions and best practices. | ä½ æ˜¯ä¸€ä½è³‡æ·±ç¨‹å¼ç¢¼å¯©æŸ¥å“¡ï¼Œå°ˆé•·åœ¨æ–¼æ‰¾å‡ºæŠ€è¡“å‚µã€é‡æ§‹å»ºè­°èˆ‡ clean code å’Œ SOLID åŸå‰‡ã€‚è«‹ç‰¹åˆ¥æ³¨æ„ Python ç¨‹å¼ç¢¼çš„æ…£ä¾‹å’Œæœ€ä½³å¯¦è¸ã€‚" \
            --arg user_msg "The following is the code diff for a pull request. Please identify any technical debt, code smells, maintenance risks, security issues, and provide refactoring suggestions:\n\n$(cat diff.txt | head -c ${{ env.MAX_DIFF_SIZE }})" \
            '{
              model: $model,
              temperature: 0.7,
              messages: [
                { role: "system", content: $system_msg },
                { role: "user", content: $user_msg }
              ]
            }')

          echo "ğŸŸ¡ Sending this request to OpenAI:"
          echo "$REQUEST_DATA" | jq . # Display the request data sent (prettified output) | é¡¯ç¤ºå‚³é€çš„è«‹æ±‚è³‡æ–™ï¼ˆç¾åŒ–è¼¸å‡ºï¼‰

          # Call OpenAI API using curl | ä½¿ç”¨ curl å‘¼å« OpenAI API
          RESPONSE=$(curl https://api.rdsec.trendmicro.com/prod/aiendpoint/v1/chat/completions \
            -s \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer ${{ secrets.OPENAI_API_KEY }}" \
            -H "extra-parameters: pass-through" \
            -d "$REQUEST_DATA")
          
          # Check if API call was successful | æª¢æŸ¥ API å‘¼å«æ˜¯å¦æˆåŠŸ
          if [ $? -ne 0 ] || [ -z "$RESPONSE" ] || [[ "$RESPONSE" == *"error"* ]]; then
            echo "api_error=true" >> $GITHUB_OUTPUT
            echo "Error calling OpenAI API or receiving response" > review.txt
            echo "$RESPONSE" >> api_error.log
          else
            echo "api_error=false" >> $GITHUB_OUTPUT
            echo "ğŸŸ¢ OpenAI Response Content: | OpenAI å›æ‡‰å…§å®¹ï¼š"
            echo "$RESPONSE" | jq . # Display OpenAI's response (prettified output) | é¡¯ç¤º OpenAI çš„å›æ‡‰ï¼ˆç¾åŒ–è¼¸å‡ºï¼‰

            # Extract review content from the response and write to review.txt | å¾å›æ‡‰ä¸­æå–å¯©æŸ¥å…§å®¹ä¸¦å¯«å…¥ review.txt
            echo "$RESPONSE" | jq -r '.choices[0].message.content' > review.txt
          fi

          # Process newlines for correct display in GitHub comments | è™•ç†æ›è¡Œç¬¦ï¼Œä»¥ä¾¿åœ¨ GitHub è©•è«–ä¸­æ­£ç¢ºé¡¯ç¤º
          REVIEW_CONTENT=$(cat review.txt | perl -pe 's/\\n/\n/g')
          # Set the review content as the step's output | å°‡å¯©æŸ¥å…§å®¹è¨­å®šç‚ºæ­¥é©Ÿçš„è¼¸å‡º
          echo "review<<EOF" >> $GITHUB_OUTPUT
          echo "$REVIEW_CONTENT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Comment on PR # Step: Comment on PR | æ­¥é©Ÿï¼šåœ¨ PR ä¸Šè©•è«–
        uses: peter-evans/create-or-update-comment@v3 # Use a third-party Action to create or update a comment | ä½¿ç”¨ç¬¬ä¸‰æ–¹ Action å»ºç«‹æˆ–æ›´æ–°è©•è«–
        with:
          token: ${{ secrets.GITHUB_TOKEN }} # Use the GitHub-provided Token for authentication | ä½¿ç”¨ GitHub æä¾›çš„ Token é€²è¡Œèº«ä»½é©—è­‰
          issue-number: ${{ github.event.pull_request.number }} # PR number for the comment | è©•è«–çš„ PR ç·¨è™Ÿ
          body: | # Comment body | è©•è«–å…§å®¹
            ${{ env.REVIEW_TITLE }}

            ${{ steps.diff.outputs.is_large == 'true' && 'âš ï¸ **Note**: This PR is large, so only the most important files were analyzed. | âš ï¸ **æ³¨æ„**ï¼šæ­¤ PR éå¤§ï¼Œåƒ…åˆ†æäº†æœ€é‡è¦çš„æª”æ¡ˆã€‚' || '' }}
            ${{ steps.openai_review.outputs.api_error == 'true' && 'âŒ **Error**: There was an issue with the OpenAI API call. Please check the workflow logs. | âŒ **éŒ¯èª¤**ï¼šå‘¼å« OpenAI API æ™‚ç™¼ç”Ÿå•é¡Œã€‚è«‹æª¢æŸ¥å·¥ä½œæµç¨‹æ—¥èªŒã€‚' || '' }}
            
            Here are the analysis and suggestions from OpenAI ${{ env.OPENAI_MODEL }} for this PR:
            ä»¥ä¸‹æ˜¯ OpenAI ${{ env.OPENAI_MODEL }} å°æ­¤ PR çš„åˆ†æèˆ‡å»ºè­°ï¼š

            ${{ steps.openai_review.outputs.review }} # Reference the output from the OpenAI review step | å¼•ç”¨ OpenAI å¯©æŸ¥æ­¥é©Ÿçš„è¼¸å‡º
            
            ${{ steps.sonarqube-scan.outcome == 'success' && 'âœ… **SonarQube scan completed successfully** | âœ… **SonarQube æƒææˆåŠŸå®Œæˆ**' || '' }}

      - name: Run Python tests # Step: Run Python tests | æ­¥é©Ÿï¼šåŸ·è¡Œ Python æ¸¬è©¦
        id: tests
        continue-on-error: true # Allow tests to fail but continue the workflow | å…è¨±æ¸¬è©¦å¤±æ•—ä½†ç¹¼çºŒå·¥ä½œæµç¨‹
        run: |
          echo "Running Python tests..."
          # Check if pytest.ini or test directory exists | æª¢æŸ¥æ˜¯å¦æœ‰ pytest.ini æˆ– test ç›®éŒ„
          if [ -f "pytest.ini" ] || [ -d "tests" ] || [ -d "test" ]; then
            pip install pytest pytest-cov
            pytest --cov=./ --cov-report=xml || echo "test_failed=true" >> $GITHUB_OUTPUT
            echo "test_status=$?" >> $GITHUB_OUTPUT
          else
            echo "No pytest configuration or test directory found, skipping tests"
            echo "test_status=skipped" >> $GITHUB_OUTPUT
          fi

      - name: Report Test Results # Step: Report test results | æ­¥é©Ÿï¼šå ±å‘Šæ¸¬è©¦çµæœ
        if: steps.tests.outputs.test_status != 'skipped'
        uses: peter-evans/create-or-update-comment@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          issue-number: ${{ github.event.pull_request.number }}
          body: |
            ## ğŸ§ª Test Results | ğŸ§ª æ¸¬è©¦çµæœ
            
            ${{ steps.tests.outputs.test_failed == 'true' && 'âŒ **Tests failed**. Please check the workflow logs for details. | âŒ **æ¸¬è©¦å¤±æ•—**ã€‚è«‹æª¢æŸ¥å·¥ä½œæµç¨‹æ—¥èªŒä»¥ç²å–è©³ç´°è³‡è¨Šã€‚' || 'âœ… **Tests passed successfully**. | âœ… **æ¸¬è©¦æˆåŠŸé€šé**ã€‚' }}

      - name: Workflow Summary # Step: Workflow summary | æ­¥é©Ÿï¼šå·¥ä½œæµç¨‹æ‘˜è¦
        run: |
          echo "## ğŸ” PR Review Summary | PR å¯©æŸ¥æ‘˜è¦" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- PR #${{ github.event.pull_request.number }}" >> $GITHUB_STEP_SUMMARY
          echo "- Diff size: ${{ steps.diff.outputs.diff_size }} bytes" >> $GITHUB_STEP_SUMMARY
          echo "- OpenAI model used: ${{ env.OPENAI_MODEL }}" >> $GITHUB_STEP_SUMMARY
          echo "- API error: ${{ steps.openai_review.outputs.api_error || 'false' }}" >> $GITHUB_STEP_SUMMARY
          echo "- Test status: ${{ steps.tests.outputs.test_status || 'not run' }}" >> $GITHUB_STEP_SUMMARY
